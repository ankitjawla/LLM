@inproceedings{vaswani2017attention,
  title     = {Attention Is All You Need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2017},
  url       = {https://arxiv.org/abs/1706.03762}
}

@article{brown2020gpt3,
  title   = {Language Models are Few-Shot Learners},
  author  = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and others},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year    = {2020},
  url     = {https://arxiv.org/abs/2005.14165}
}

@article{chowdhery2022palm,
  title   = {PaLM: Scaling Language Modeling with Pathways},
  author  = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and others},
  journal = {arXiv preprint},
  year    = {2022},
  url     = {https://arxiv.org/abs/2204.02311}
}

@article{shazeer2017moe,
  title   = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author  = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal = {arXiv preprint},
  year    = {2017},
  url     = {https://arxiv.org/abs/1701.06538}
}

@article{fedus2021switch,
  title   = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author  = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal = {arXiv preprint},
  year    = {2021},
  url     = {https://arxiv.org/abs/2101.03961}
}

@inproceedings{du2022glam,
  title     = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  author    = {Du, Nan and Huang, Yanping and Dai, Andrew M. and others},
  booktitle = {Proceedings of ICML},
  year      = {2022},
  url       = {https://arxiv.org/abs/2112.06905}
}

@article{beltagy2020longformer,
  title   = {Longformer: The Long-Document Transformer},
  author  = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  journal = {arXiv preprint},
  year    = {2020},
  url     = {https://arxiv.org/abs/2004.05150}
}

@inproceedings{zaheer2020bigbird,
  title     = {Big Bird: Transformers for Longer Sequences},
  author    = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and others},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.14062}
}

@inproceedings{kitaev2020reformer,
  title     = {Reformer: The Efficient Transformer},
  author    = {Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.04451}
}

@article{roy2020routing,
  title   = {Efficient Content-Based Sparse Attention with Routing Transformers},
  author  = {Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal = {arXiv preprint},
  year    = {2020},
  url     = {https://arxiv.org/abs/2003.05997}
}

@article{choromanski2021performer,
  title   = {Rethinking Attention with Performers},
  author  = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and others},
  journal = {arXiv preprint},
  year    = {2020},
  url     = {https://arxiv.org/abs/2009.14794}
}

@article{wang2020linformer,
  title   = {Linformer: Self-Attention with Linear Complexity},
  author  = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal = {arXiv preprint},
  year    = {2020},
  url     = {https://arxiv.org/abs/2006.04768}
}

@article{dao2022flashattention,
  title   = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author  = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Re, Christopher},
  journal = {arXiv preprint},
  year    = {2022},
  url     = {https://arxiv.org/abs/2205.14135}
}

@inproceedings{borgeaud2022retro,
  title     = {Improving Language Models by Retrieving from Trillions of Tokens},
  author    = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and others},
  booktitle = {Proceedings of ICML},
  year      = {2022},
  url       = {https://arxiv.org/abs/2112.04426}
}

@article{dai2019transformerxl,
  title   = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  author  = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  journal = {arXiv preprint},
  year    = {2019},
  url     = {https://arxiv.org/abs/1901.02860}
}

@article{press2021alibi,
  title   = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author  = {Press, Ofir and Smith, Noah A. and Lewis, Mike},
  journal = {arXiv preprint},
  year    = {2021},
  url     = {https://arxiv.org/abs/2108.12409}
}

@article{su2021roformer,
  title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author  = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal = {arXiv preprint},
  year    = {2021},
  url     = {https://arxiv.org/abs/2104.09864}
}

@inproceedings{yun2019universal,
  title     = {Are Transformers Universal Approximators of Sequence-to-Sequence Functions?},
  author    = {Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J. and Kumar, Sanjiv},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2020},
  url       = {https://arxiv.org/abs/1912.10077}
}

@inproceedings{yun2020sparseuniversal,
  title     = {Universal Approximability of Sparse Transformers},
  author    = {Yun, Chulhee and Chang, Hyokun and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J. and Kumar, Sanjiv},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/file/9ed27554c893b5bad850a422c3538c15-Paper.pdf}
}

@misc{jawla2025ui,
  title        = {LLM Repository UI Snapshot (homepage-light.png)},
  author       = {Jawla, Ankit},
  howpublished = {\url{https://github.com/ankitjawla/LLM/blob/master/images/themes/homepage-light.png}},
  year         = {2025},
  note         = {Accessed: use local download for LaTeX build}
}
